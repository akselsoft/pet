---
layout: default
title: "Sound Discrimination Walkthrough"
---

This walkthrough demonstrates how a PET-based system learns to distinguish and classify sounds, beginning from raw audio input and evolving toward meaning and emotional response.

The system starts by detecting basic soundwave patterns—tones, frequencies, and durations—captured by the auditory sensor. These patterns are stored in the short-term graph and matched against previously encountered auditory features using the KeyPair index.

Through repeated exposure, the system begins to cluster certain patterns—like a soothing lullaby, a sudden loud crash, or a repeated spoken word—into higher-level auditory concepts. These are then linked to **Understanding Nodes** that record the source, experience, and outcome of each sound (e.g., “Clap → Surprise” or “Mother’s voice → Comfort”).

This walkthrough illustrates:

- The association of raw sensory input with meaningful labels.
- How repeated pattern exposure leads to classification.
- The emergence of emotional and contextual associations with specific sounds.
